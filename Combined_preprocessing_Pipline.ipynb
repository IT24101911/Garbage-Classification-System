{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Combined Preprocessing Pipeline**\n"
      ],
      "metadata": {
        "id": "BXPQyaTh59I_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UItmcnhK9eWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install opendatasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4znYOmlNSH1M",
        "outputId": "8a8adb16-ff55-42c5-8c14-fa9442b1ac00"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.12/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from opendatasets) (8.2.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.4.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/zlatan599/garbage-dataset-classification\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPHbAvUfpf55",
        "outputId": "bd45b02e-3baa-42b2-83ef-31f4b51d2de4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./garbage-dataset-classification\" (use force=True to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Sl05Cu6Y93wE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load training and validation data and resize the images**"
      ],
      "metadata": {
        "id": "GW2gx7eg-NSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Rescaling\n",
        "\n",
        "data_dir = \"/content/garbage-dataset-classification/Garbage_Dataset_Classification/\"\n",
        "\n",
        "# Create datasets (80/20 split)\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfZhQbX2RD92",
        "outputId": "eb69e6f3-4c3f-47b5-981b-3f06a27e755f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 13901 files belonging to 1 classes.\n",
            "Using 11121 files for training.\n",
            "Found 13901 files belonging to 1 classes.\n",
            "Using 2780 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rescaling**"
      ],
      "metadata": {
        "id": "TYiSBuZMY6EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(lambda x, y: (Rescaling(1./255)(x), y))\n",
        "val_ds   = val_ds.map(lambda x, y: (Rescaling(1./255)(x), y))"
      ],
      "metadata": {
        "id": "AbO9ijN7YZyE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Geometric Augmentation**"
      ],
      "metadata": {
        "id": "xxTGIbJUZJoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "geometric_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    layers.RandomRotation(0.2),\n",
        "    layers.RandomZoom(0.2),\n",
        "    layers.RandomTranslation(0.1, 0.1),\n",
        "])\n",
        "\n",
        "def apply_geometric_augmentation(image, label):\n",
        "    image = geometric_augmentation(image, training=True)\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(apply_geometric_augmentation)\n"
      ],
      "metadata": {
        "id": "JjC536XqZUex"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Color_Augmentation**"
      ],
      "metadata": {
        "id": "G2htIpWCZ6Bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "color_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomBrightness(0.2),\n",
        "    layers.RandomContrast(0.2),\n",
        "    layers.RandomSaturation(0.2),\n",
        "    layers.RandomHue(0.1),\n",
        "])\n",
        "\n",
        "def apply_color_augmentation(image, label):\n",
        "    image = color_augmentation(image, training=True)\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(apply_color_augmentation)"
      ],
      "metadata": {
        "id": "FEaBdbXDaFES"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gaussian Noise**"
      ],
      "metadata": {
        "id": "nvkSIh1caWiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import GaussianNoise\n",
        "\n",
        "def add_gaussian_noise(image, stddev=0.05):\n",
        "    noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=stddev)\n",
        "    return tf.clip_by_value(image + noise, 0.0, 1.0)\n",
        "\n",
        "def apply_noise_augmentation(image, label):\n",
        "    image = add_gaussian_noise(image)\n",
        "    return image,label\n",
        "\n",
        "train_ds = train_ds.map(apply_noise_augmentation)\n"
      ],
      "metadata": {
        "id": "1zGo9_CmacPh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Histogram Equalization**"
      ],
      "metadata": {
        "id": "m0QejuNSaniZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def histogram_equalization_rgb(image):\n",
        "\n",
        "    # Convert to YUV color space\n",
        "    image_yuv = tf.image.rgb_to_yuv(image)\n",
        "\n",
        "    # Apply histogram equalization to Y channel (luminance)\n",
        "    y_channel = image_yuv[..., 0:1]\n",
        "\n",
        "    # Enhanced contrast adjustment (simulating histogram equalization)\n",
        "    y_eq = tf.image.adjust_contrast(y_channel, 2.0)\n",
        "    y_eq = tf.clip_by_value(y_eq, 0.0, 1.0)\n",
        "\n",
        "    # Combine back with original UV channels\n",
        "    image_yuv_eq = tf.concat([y_eq, image_yuv[..., 1:2], image_yuv[..., 2:3]], axis=-1)\n",
        "\n",
        "    # Convert back to RGB\n",
        "    return tf.image.yuv_to_rgb(image_yuv_eq)\n",
        "\n",
        "def apply_histogram_equalization(image, label):\n",
        "    image = histogram_equalization_rgb(image)\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(apply_histogram_equalization)\n"
      ],
      "metadata": {
        "id": "mQuQlbaUarcg"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}